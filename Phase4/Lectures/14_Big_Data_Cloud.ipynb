{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data and Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Understand what is \"big data\"\n",
    "- Know why big data is different and how it can be processed\n",
    "- Understand how data can be handled in distributed and parallel systems\n",
    "- Understand how MapReduce is run\n",
    "- Explain the general concept of \"the cloud\"\n",
    "- Understand the cases where ***hardware acceleration*** is useful\n",
    "- Understand the cases where ***cloud storage*** and the **Boto3** library in particular are useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# What is Big Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> A different amount makes a different kind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There is no clear/agreed upon definition but typically we say we're working on **big data** if we have to use something like a distributed computing system (not just one local machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Interactive numbers: https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Thrashing**: when your CPU is bored, waiting for tasks to do since it has to wait for their slowpoke friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The 3 Vs of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "+ Volume --> Large Amounts\n",
    "+ Velocity --> Quickly Generated\n",
    "+ Variety --> Unstructured "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/3vs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Data is *big* when it is better/faster to split the work over the network amongst more (parallel) because of one or more of these Vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Applying it via Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Hadoop Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/hadoop_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Considered \"old-school\"\n",
    ">\n",
    "> Slower since it has to write to disk each time\n",
    "\n",
    "- Storage (usually HDFS) \n",
    "- Data Processing (MapReduce)\n",
    "- Resource Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/apache_spark_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Holds data in memory whenever possible (faster)\n",
    ">\n",
    "> Can still be built on top of Hadoop but also S3 on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Spark has become king of data since it does a good job with ETL (Extract-Transform-Load) & ML in distributed systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### _Aside: More Detail on Spark_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Some Resources**\n",
    "\n",
    ">[Here](https://towardsdatascience.com/a-neanderthals-guide-to-apache-spark-in-python-9ef1f156d427) is a great walkthrough of Spark basics!\n",
    ">\n",
    "> And [here](https://towardsdatascience.com/apache-spark-a-conceptual-orientation-e326f8c57a64)'s another from our very own alum, Alex Shropshire!\n",
    ">\n",
    "> Spark has APIs for Scala (this is ur-Spark), Java, Python, and R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "N.B. Unless otherwise marked, page references are to [Salloum, Dautov, et al., \"Big Data Analytics on Apache Spark\", 2016](https://link.springer.com/content/pdf/10.1007%2Fs41060-016-0027-9.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Spark is a tool for the management of big data. Sometimes data science professionals will refer to the [five \"V\"s](https://www.bbva.com/en/five-vs-big-data/) of big data. Clearly, the availabilty and size of datasets are growing rapidly. What counts as \"big data\"? Roughly speaking, we're talking about datasets that are too large to be processed on a single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Many large companies are relying on big data these days, and Spark is a major player in the big data game. Examples can be found [here](https://www.icas.com/news/10-companies-using-big-data) and [here](https://enlyft.com/tech/products/apache-spark) and [here](https://www.quora.com/Which-are-the-companies-that-use-apache-spark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So ... how in the world *do* you process a dataset that's too large for a single machine? You use multiple machines linked together! Let's call each machine a *node*, and the group of all machines working in parallel a *cluster*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The origin story of Spark starts with [MapReduce](https://en.wikipedia.org/wiki/MapReduce), whose programs comprise (unsurprisingly) a \"map\" routine (for filtering and sorting) and a \"reduce\" routine (for performing some aggregate operation).\n",
    "\n",
    "Let's look at an [example](https://en.wikipedia.org/wiki/MapReduce#Logical_view):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An early major player in big data that used MapReduce was [Hadoop](https://en.wikipedia.org/wiki/Apache_Hadoop). Hadoop was (and still is) a framework for distributed data processing. Its processing component used MapReduce, but it also had a storage component called the \"Hadoop Distributed File System\".\n",
    "\n",
    "From Wikipedia: \"Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers packaged code into nodes to process the data in parallel. This approach takes advantage of data locality, where nodes manipulate the data they have access to. This allows the dataset to be processed faster and more efficiently than it would be in a more conventional supercomputer architecture that relies on a parallel file system where computation and data are distributed via high-speed networking\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But Spark appeared as open source in 2010, and had some advantages over Hadoop MapReduce.\n",
    "\n",
    "Spark's advances over Hadoop MapReduce:\n",
    "\n",
    "- data processing in memory rather than on disks\n",
    "- a single framework for machine learning, graph analysis, and processing of streaming data (pp. 159-160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For more on the advantages of Spark over MapReduce, see [this piece](https://research.ijcaonline.org/volume113/number1/pxc3900531.pdf).\n",
    "\n",
    "Distributed computing can help enormously with speed. Check out [this website](http://sortbenchmark.org) for the latest in speed records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\"As a framework, it combines a core engine for distributed computing with an advanced programming model for in-memory processing. Although it has the same linear scalability and fault tolerance capabilities as those of MapReduce, it comes with a multistage in-memory programming model comparing to the rigid map-then-reduce disk-based model\" (146).\n",
    "\n",
    "Illustration, p. 148, of Spark guts.\n",
    "\n",
    "\"Running a Spark application involves five key entities ... a driver program, a cluster manager, workers, executors and tasks. A driver program is an application that uses Spark as a library and defines a high-level control flow of the target computation. While a worker provides CPU, memory and storage resources to a Spark application, an executer \\[sic\\] is a JVM (Java Virtual Machine) process that Spark creates on each worker for that application. A job is a set of computations (e.g., a data processing algorithm) that Spark performs on a cluster to get results to the driver program. A Spark application can launch multiple jobs. Spark splits a job into a directed acyclic graph (DAG) of stages where each stage is a collection of tasks. A task is the smallest unit of work that Spark sends to an executor. The main entry point for Spark functionalities is a SparkContext through which the driver program access \\[sic\\] Spark. A SparkContext represents a connection to a computing cluster\" (149)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "RDDs, Transformations, and Actions:\n",
    "\n",
    "Fault tolerance achieved by keeping a record of the RDD's lineage. There are *redundancies* in the data records, so that, in the event of node failure, the other nodes can provide for data recovery. This is what makes these RDDs *resilient*.\n",
    "\n",
    "- Transformations take one from an RDD to another RDD;\n",
    "- Actions take one from an RDD to an output value.\n",
    "\n",
    "Broadcast variables and accumulators act as global variables; the latter are for counters or sums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Surveys of Big Data tools [here](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-015-0032-1) and [here](https://ieeexplore.ieee.org/document/7300948).\n",
    "\n",
    "Debugging can be a challenge in Spark. [This project](https://sites.google.com/site/sparkbigdebug/) was started to help with that.\n",
    "\n",
    "Also check out Paco Nathan's [massive slide show presentation](http://stanford.edu/~rezab/sparkclass/slides/itas_workshop.pdf) on Spark. Let's just look at slides 66-7 and 82."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Aside: The story of Spark (a timeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "|<p align=\"left justify\">Date</p>|<p align=\"left justify\">Product</p>|<p align=\"left justify\">Update</p>|\n",
    "|:----|:-----|:-----|\n",
    "| 2002 | Hadoop | <p align=\"left justify\">Doug Cutting starts `Apache Nutch` researching sort/merge processing</p> |\n",
    "| 2006 | Hadoop |  <p align=\"left justify\">Leaves `Nutch` and joins `Yahoo`, renaming the project `Hadoop` </p>|\n",
    "| 2008 | Hadoop |  <p align=\"left justify\">`Hadoop` was made `Apache’s` top level project </p> |\n",
    "| Jan 2008 | Hadoop |  <p align=\"left justify\">v 0.10.1 released </p>|\n",
    "| 2009 | Spark | <p align=\"left justify\">started as a research project at the UC Berkeley AMPLab  </p>|\n",
    "| 2010 | Spark |  <p align=\"left justify\">open sourced </p>|\n",
    "| Sept 2012 | Spark |  <p align=\"left justify\">0.6.0 released </p>|\n",
    "| 2013 | Spark |  <p align=\"left justify\">moved to the `Apache` Software Foundation </p>|\n",
    "| Feb 2013| Spark |  <p align=\"left justify\">Spark 0.7 adds a Python API called `PySpark` </p>|\n",
    "| Sept 2013 | Spark | <p align=\"left justify\">0.8.0 introduces `MLlib` </p>|\n",
    "| 2013 | Databricks |  <p align=\"left justify\">Original Spark research team at UC Berkeley found Databricks</p> |\n",
    "| May 2014 |Spark |  <p align=\"left justify\">v 1.0 introduces Spark SQL, for loading and manipulating structured data in Spark</p>|\n",
    "| Sept 2014 | Spark|  <p align=\"left justify\">v 1.1.0 provided support for registering Python lambda funtions as UDFs</p>|\n",
    "|Mar 2015 | Spark | <p align=\"left justify\"> v 1.3.0 brings a new DataFrame API</p> |\n",
    "| Jun 2015 | Spark | <p align=\"left justify\"> v 1.4.0 brings an R API to Spark</p> |\n",
    "| 2015 | Databricks | <p align=\"left justify\"> The Databricks Apache Spark cloud platform goes public</p> |\n",
    "| Jan 2016|  Spark | <p align=\"left justify\"> v 1.6.0 brings a new Dataset API <br> - A new Spark API, similar to RDDs, that allows users to work with custom objects and lambda functions while still gaining the benefits of the Spark SQL execution engine.</p> |\n",
    "| Jul 2016 | Spark | <p align=\"left justify\"> v 2.0.0 **big update**! <Br> - Unifying DataFrame and Dataset: In Scala and Java, DataFrame and Dataset have been unified, i.e. DataFrame is just a type alias for Dataset of Row. In Python and R, given the lack of type safety, DataFrame is the main programming interface. <br> - SparkSession: new entry point that replaces the old SQLContext<br>- Native CSV data source, based on Databricks’ spark-csv module<br>- MLlib - The DataFrame-based API is now the primary API. The RDD-based API is entering maintenance mode </p> |\n",
    "| 2016 | Databricks | <p align=\"left justify\"> Databricks Launches Free Community Edition As Companion To Free Online Spark Courses </p>|\n",
    "| Jul 2017| Spark | <p align=\"left justify\"> v 2.2.0 drops support for Python 2.6 |\n",
    "| Nov 2018 | Spark | <p align=\"left justify\"> v 2.4.0<br> - This release adds Barrier Execution Mode for better integration with deep learning frameworks<br> - more integration between pandas UDF and spark DataFrames </p>|\n",
    "| June 2020 | Spark | 3.0 <p align=\"left justify\"> - This release adds adaptive query execution <br> - ANSI SQL compliance <br> - pandas API improvements|\n",
    "| March 2021 | Spark| 3.1.2 <p align=\"left justify\"> - Stability Fixes</p>|\n",
    "| October 2021 | Spark | 3.2.0 <p align=\"left justify\"> - Added support for pandas API</p> |\n",
    "| June 2022 | Spark | 3.3.0 <p align=\"left justify\"> - Increased join query performance<br>- Extends panda API coverage</p> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Spark Data Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***In Pyspark there are only RDD and DataFrames***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In other languages where \"compiling\" is done, there is the distinction between DataFrames and DataSet. \n",
    "\n",
    "![dataframe image](https://databricks.com/wp-content/uploads/2018/05/DataFrames.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Use an RDD when:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[quoted from databricks](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n",
    "\n",
    "> - you want low-level transformation and actions and control on your dataset;\n",
    "> - your data is unstructured, such as media streams or streams of text;\n",
    "> - you want to manipulate your data with functional programming constructs than domain specific expressions;\n",
    "> - you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Use a dataframe when:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[also quoted from databricks](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> - you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame\n",
    "> - your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame\n",
    "> - you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten’s efficient code generation, use Dataset.\n",
    "> - you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.\n",
    "> - If you are a R user, use DataFrames.\n",
    "> - If you are a Python user, use DataFrames and resort back to RDDs if you need more control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Note**: Machine learning algorithms are run on _DataFrames_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## But Spark Isn't Always the Best Tool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/tech_stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# What Do We Mean by \"Parallel\" & \"Distributed\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/types_of_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> tasks split up and executed by different workers\n",
    "\n",
    "+ Multiple CPUs each have their own memory\n",
    "+ Multiple CPUs share via a network (using \"messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/sequential.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Take a step at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/parallel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> executing tasks in a non-sequential order\n",
    "\n",
    "+ Multiple CPUs share same memory to \"communicate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Aside: MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Describes two jobs: **Map** & **Reduce**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Software best for **clusters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/MapReduceZooExample.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Steps in MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/mapreduce_visual.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nerds.net/wp-content/uploads/2018/02/cloud-computer-reality-750x646.jpg\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is \"The Cloud\"?\n",
    "\n",
    "For ***computationally intensive*** or ***long-running*** tasks, it doesn't make much sense to use a personal computer. Personal computers are not particularly powerful, and you also might want to turn them off or use their computing power to do other things.\n",
    "\n",
    "<a title=\"GNOME Project, CC BY-SA 3.0 &lt;https://creativecommons.org/licenses/by-sa/3.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Gnome-computer.svg\"><img width=\"240\" alt=\"Gnome-computer\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Gnome-computer.svg/240px-Gnome-computer.svg.png\"></a>\n",
    "\n",
    "Before the cloud, organizations would typically have ***on-premises dedicated hardware*** for these tasks. This meant that they also needed IT systems administrators to manage the physical hardware.\n",
    "\n",
    "<a title=\"Akramusns, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Unique_Server_Racks.png\"><img width=\"512\" alt=\"Unique Server Racks\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/db/Unique_Server_Racks.png/512px-Unique_Server_Racks.png\"></a>\n",
    "\n",
    "<small><i>Server racks used for web hosting (2014).</i></small>\n",
    "\n",
    "With cloud computing, ***hardware details are abstracted away*** and you can get on-demand computing power. The code is still running on a server maintained by the cloud provider, but you don't need an IT systems administrator to coordinate how the servers will be used.\n",
    "\n",
    "<a title=\"百楽兎, CC BY-SA 3.0 &lt;https://creativecommons.org/licenses/by-sa/3.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Cloud_computing_icon.svg\"><img width=\"320\" alt=\"Cloud computing icon\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Cloud_computing_icon.svg/320px-Cloud_computing_icon.svg.png\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud Providers\n",
    "\n",
    "Below is a graph showing some of the top cloud providers used by enterprise customers today:\n",
    "\n",
    "![public cloud adoption for enterprises](https://www.flexera.com/blog/wp-content/uploads/2021/03/Picture9.png)\n",
    "\n",
    "### Choosing a Cloud Provider\n",
    "\n",
    "***On the job*** there will likely already be a preferred cloud provider that your employer uses, so you won't need to make a decision here. But ***as a student*** here are some things to consider:\n",
    "\n",
    "#### Big-Name Providers\n",
    "\n",
    "Consider choosing to use one of the most popular providers, because this may help you in the job search.\n",
    "\n",
    "<img src=\"https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png\" width=350 alt=\"aws logo\"/>\n",
    "\n",
    "**AWS** (Amazon Web Services) is currently the most popular cloud provider. In a previous Flatiron School analysis of the job market, we found that about 6% of entry-level Data Scientist roles specifically mentioned AWS as a required skill. AWS was the first true \"cloud services\" provider -- launching Simple Storage Service (S3) and Elastic Compute Cloud (EC2) in 2006, and is still very popular in part because they were the first of their kind. Check out this [Introduction to AWS for Data Scientists](https://www.dataquest.io/blog/introduction-to-aws-for-data-scientists/) for more information on navigating all of the available services.\n",
    "\n",
    "<a title=\"Microsoft Corporation, Public domain, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Microsoft_Azure_Logo.svg\"><img width=\"320\" alt=\"Microsoft Azure Logo\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a8/Microsoft_Azure_Logo.svg/320px-Microsoft_Azure_Logo.svg.png\"></a>\n",
    "\n",
    "**Microsoft Azure** (also just referred to as Azure) is the second most popular cloud provider. Our analysis found that about 2% of entry-level Data Scientist roles specifically mentioned Azure as a required skill. Azure launched later than AWS, and has very good compatibility with Windows tools and software.\n",
    "\n",
    "<img src=\"https://www.vectorlogo.zone/logos/google_cloud/google_cloud-ar21.png\" width=350 alt=\"google cloud logo\" />\n",
    "\n",
    "**Google Cloud** is the third most popular cloud provider. It did not appear in our entry-level Data Scientist role analysis as a requirement. It also launched later than AWS, and is compatible with other Google products.\n",
    "\n",
    "#### Cost\n",
    "\n",
    "In general, more expensive services will perform better than cheaper services:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/630/1*Ao2QhhVEBEr2mJXL9jsuWQ.png\" alt=\"cost per hour vs. time to train\"/>\n",
    "<small><i>From <a href=\"https://towardsdatascience.com/maximize-your-gpu-dollars-a9133f4e546a\" target=\"_blank\">Best Deals in Deep Learning Cloud Providers</a></i></small>\n",
    "\n",
    "As a student, you have no obligation to pay anything for cloud services! We are just letting you know that they exist, and what they can do for you.\n",
    "\n",
    "Some cloud services offer a 100% **free version** where you do not need to enter a credit card. These include:\n",
    "\n",
    "* [Google Colab](https://research.google.com/colaboratory/)\n",
    "* [Kaggle Kernels](https://www.kaggle.com/code)\n",
    "* [Databricks Community Edition](https://databricks.com/product/faq/community-edition)\n",
    "* [MongoDB Atlas](https://www.mongodb.com/docs/atlas/tutorial/deploy-free-tier-cluster/)\n",
    "* [Heroku](https://www.heroku.com/)\n",
    "\n",
    "Other services will offer **free credits**. This includes AWS, Azure, Google Cloud, and others. They usually offer a default amount of free credit but will occasionally have special promotions for additional credit.\n",
    "\n",
    "To use free credits, you will typically need to enter credit card information, so make sure you pay attention to your free credit balance so you don't spend money that you don't intend to spend!\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Would a Data Scientist Use Cloud Services?\n",
    "\n",
    "The two main reasons a data scientist would use cloud services are to ***get more computing power*** and to ***deploy machine learning models***.\n",
    "\n",
    "## More Computing Power\n",
    "\n",
    "Particularly with large datasets and tools like grid search that fit many different model iterations, training a model can take a **long time** on a personal computer. Maybe you have already had the experience of running a model and having to step away from the computer for minutes or even hours as the fan spins and the computer works hard to perform all of the necessary computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware Acceleration\n",
    "\n",
    "<a title=\"Nick Stathas, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Under_the_GPU.jpg\"><img width=\"256\" alt=\"Under the GPU\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Under_the_GPU.jpg/256px-Under_the_GPU.jpg\"></a>\n",
    "\n",
    "<small><i>A zoomed-in photo of the capacitors inside of a GPU.</i></small>\n",
    "\n",
    "As much as software libraries like NumPy or Spark can improve the efficiency of code, there is a limit to how much of a difference they can make, depending on the actual hardware of your computer.\n",
    "\n",
    "As a general concept, [hardware acceleration](https://www.omnisci.com/learn/resources/technical-glossary/hardware-acceleration) means using purpose-built hardware rather than general-purpose hardware.\n",
    "\n",
    "In the case of machine learning, this typically means running your code on a **GPU**, rather than a CPU.  A CPU _can_ do everything that a GPU can do, but it is not optimized for it, so it will likely take more time.  [This blog](https://towardsdatascience.com/maximize-your-gpu-dollars-a9133f4e546a) argues that a CPU is to a GPU as a horse and buggy is to a car.\n",
    "\n",
    "One approach you might take would be to purchase a more powerful computer, with a GPU, more RAM, etc. and just use it for training models. But that can easily get very expensive!\n",
    "\n",
    "With a cloud service, you can train a machine learning model using GPU hardware, so the training should complete much more quickly than on a typical personal computer. And unlike having a dedicated computer, you're only paying for the computing power when you need it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Instances/Containers with GPUs\n",
    "\n",
    "<a title=\"Unknown authorUnknown author, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:SSH_diagram.png\"><img width=\"512\" alt=\"SSH diagram\" src=\"https://upload.wikimedia.org/wikipedia/commons/c/c8/SSH_diagram.png\"></a>\n",
    "\n",
    "<small><i>SSH diagram</i></small>\n",
    "\n",
    "A cloud instance means you can run a customized, fully-fledged computer in the cloud. This often gives you the most fine-grained control but can also be much more expensive because they are not designed specifically for machine learning. Typically you will need to connect to a cloud instance via SSH, and you'll need to be comfortable navigating in a terminal interface.\n",
    "\n",
    "AWS Elastic Compute Cloud (EC2) is probably the most well-known cloud instance, and our analysis found that it was mentioned in about 2% of entry-level Data Scientist job postings.\n",
    "\n",
    "Here are some cloud container options with GPUs:\n",
    "\n",
    " - [AWS EC2](https://aws.amazon.com/blogs/machine-learning/train-deep-learning-models-on-gpus-using-amazon-ec2-spot-instances/)\n",
    " - [Google Cloud Platform](https://cloud.google.com/ml-engine/docs/using-gpus)\n",
    " - [IBM Watson Studio](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml_dlaas_gpus.html)\n",
    " - [Azure VM](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-gpu)\n",
    " - [Oracle Cloud](https://www.oracle.com/cloud/compute/gpu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Notebooks\n",
    "\n",
    "<img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2019/12/01/Rhinestone-SageMaker-Studio-Page-2-v2.png\" width=500 alt=\"amazon sagemaker studio\"/>\n",
    "\n",
    "<small><i>From <b>Amazon SageMaker Studio: The First Fully Integrated Development Environment For Machine Learning</b> on the <a href=\"https://aws.amazon.com/blogs/aws/amazon-sagemaker-studio-the-first-fully-integrated-development-environment-for-machine-learning/\" target=\"blank_\">AWS News Blog</a></i></small>\n",
    "\n",
    "Compared to virtual machines, cloud notebooks tend to be easier to work with because they allow you to use the familiar notebook interface. Some of them even have free GPUs or TPUs! Even without hardware acceleration, cloud notebooks can allow you to train models in the cloud and free up resources on your personal computer to do other tasks.\n",
    "\n",
    "Here are some cloud notebooks to consider:\n",
    "\n",
    " - [AWS Sagemaker](https://aws.amazon.com/machine-learning/accelerate-machine-learning-P3/)\n",
    " - [Databricks Community Edition](https://community.cloud.databricks.com/)\n",
    " - [Google Colab](https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c)\n",
    " - [Kaggle kernels](https://www.kaggle.com/dansbecker/running-kaggle-kernels-with-a-gpu)\n",
    " - [data.world](https://jupyter.data.world/)\n",
    " \n",
    "Because there is a GPU available in the free tier, Google Colab is the most popular of these tools for our students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Storage\n",
    "\n",
    "<a title=\"Kottakkalnet, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Plastic_bucket_IMG_20160701_161628956.jpg\"><img width=\"512\" alt=\"Plastic bucket IMG 20160701 161628956\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b8/Plastic_bucket_IMG_20160701_161628956.jpg/512px-Plastic_bucket_IMG_20160701_161628956.jpg\"></a>\n",
    "\n",
    "It's annoying to have huge data files taking up space on your laptop, and if you want to train your model in the cloud, your data also needs to be in the cloud.  But for reasons related to hardware acceleration, it can get pretty expensive to store large datasets in general-purpose cloud services like an EC2 instance or a cloud VM.  That's when cloud storage services become useful.\n",
    "\n",
    "#### Cloud Storage Buckets\n",
    "\n",
    "The major providers of storage \"buckets\" are:\n",
    "\n",
    " - [AWS S3](https://aws.amazon.com/s3/getting-started/)\n",
    " - [Google Cloud Storage](https://cloud.google.com/storage/)\n",
    " - [Azure Storage](https://docs.microsoft.com/en-us/azure/storage/common/storage-introduction)\n",
    "\n",
    "These tools are designed for uploads of raw files, e.g. folders full of images, CSVs, or JSONs.\n",
    "\n",
    "They each cost about 2-5 cents per GB per month.  AWS S3 is the oldest and tends to have the most integration support with other platforms, although you may need to use Google storage if you're using other Google products or Azure storage if you're using other Azure products.\n",
    "\n",
    "**Boto3** is the Python library used to connect to S3, and there is a demonstration of how to use it in the \"Level Up\" portion of this notebook.\n",
    "\n",
    "#### Cloud Databases\n",
    "\n",
    "If you want to deploy a website where new information gets saved (what kinds of queries users perform, user ratings of the quality of predictions, etc.) then you need a cloud database.  These work roughly the same as a database running on your computer.\n",
    "\n",
    "Using a cloud database is mainly an opportunity to practice using tooling that you are likely to use on the job, because they assist with collaboration.\n",
    "\n",
    "Some popular providers are:\n",
    "\n",
    " - [Heroku Postgres](https://www.heroku.com/postgres)\n",
    " - [MongoDB Atlas](https://www.mongodb.com/cloud/atlas)\n",
    " - [AWS Aurora](https://aws.amazon.com/rds/aurora/)\n",
    " - [AWS RDS](https://aws.amazon.com/rds/)\n",
    "\n",
    "Most of these tools have a free tier, which permits a limited number of records to be stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying ML Models\n",
    "\n",
    "Typically in this program we have used Jupyter Notebooks to build, train, and evaluate models. Jupyter Notebook is a very useful interface, but a predictive model that only exists in the context of a notebook is not particularly useful in the real world!\n",
    "\n",
    "Some key tools and techniques to be aware of for deploying ML models include model persistence (pickling), deploying as an API, and deploying as a full-stack web app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Persistence\n",
    "\n",
    "<a title=\"Renee Comet (photographer), Public domain, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Pickle.jpg\"><img width=\"256\" alt=\"Pickle\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Pickle.jpg/256px-Pickle.jpg\"></a>\n",
    "\n",
    "Recall that there is a difference between a file ***on disk*** and a variable ***in memory***. Variables in memory only persist until the notebook kernel is shut down, whereas files on disk persist as long as there is functional storage hardware.\n",
    "\n",
    "When you first train a model, it only exists in memory. The process for storing it on disk is called ***pickling***. This is a type of serialization where the trained model gets stored in a file, conventionally using a `.pkl` extension. There is an example in the \"Level Up\" section of this notebook demonstrating the fitting, pickling, and un-pickling of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying a Model as an API\n",
    "\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/request_response_cycle.png\" width=600 alt=\"client-server model and request-response cycle\"/>\n",
    "\n",
    "<small><i>Icons made by <a href=\"https://www.flaticon.com/authors/freepik\" target=\"blank_\">Freepik</a> from www.flaticon.com</i></small>\n",
    "\n",
    "Once you have a pickled model, in theory anyone who can execute Python code can then un-pickle the model and use it to make predictions.\n",
    "\n",
    "However, what if you want someone to be able to use your model even if they aren't running Python code? For example, what if the model is being used in the context of a website or a mobile app?\n",
    "\n",
    "In that case, developing an HTTP ***API*** can be useful because this protocol is compatible with many different programming languages.\n",
    "\n",
    "An example interaction with a model deployed as an API would be:\n",
    "\n",
    "1. Client request: `POST /predict '{\"sepal_length\": 5.1, \"sepal_width\": 3.5, \"petal_length\": 1.4, \"petal_width\": 0.2}'`\n",
    "2. Server response: `'{\"predicted_class\": 0}'`\n",
    "\n",
    "Just like there are multiple tools that can make a `POST` request (Python `requests` library, `cURL` in the terminal, JavaScript `fetch` in a web appication), there are multiple ways you can build your server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying a Model as an API with Cloud Functions\n",
    "\n",
    "<a title=\"Wvbailey, Public domain, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Function_machine2.svg\"><img width=\"243\" alt=\"Function machine2\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/Function_machine2.svg/243px-Function_machine2.svg.png\"></a>\n",
    "\n",
    "The most minimal way to deploy a machine learning model is to create a ***cloud function***. This means that you created a pickled model, write a few lines of Python code specifying how to un-pickle the model and use it to make predictions, and deploy it with a cloud service designed for this purpose.\n",
    "\n",
    "[This curriculum lesson](https://github.com/learn-co-curriculum/dsc-pickling-pipelines) walks through the process of developing and deploying a Google Cloud function, including the process of pickling a full pipeline. The complete Google Cloud function from that lesson looks like this:\n",
    "\n",
    "```python\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "def iris_prediction(sepal_length, sepal_width, petal_length, petal_width):\n",
    "    with open(\"model.pkl\", \"rb\") as f:\n",
    "        model = joblib.load(f)\n",
    "    X = [[sepal_length, sepal_width, petal_length, petal_width]]\n",
    "    predictions = model.predict(X)\n",
    "    prediction = int(predictions[0])\n",
    "    return {\"predicted_class\": prediction}\n",
    "\n",
    "def predict(request):\n",
    "    request_json = request.get_json()\n",
    "    result = iris_prediction(**request_json)\n",
    "    return json.dumps(result)\n",
    "```\n",
    "\n",
    "While they involve writing the least amount of code of any model deployment option, cloud functions can be tricky to configure within the cloud service. Looking at the code above you might notice that the `predict` function is never actually invoked in the code -- when you configure the cloud function, you have to specify that this function should be called. You will also need to configure the cloud function so that it can accept public web requests, and typically you won't be able to test anything on your local computer, so this can be a slow back-and-forth of tweaking the configuration until it works.\n",
    "\n",
    "We found that the Google Cloud functions were the easiest to work with, but you also might want to check out [AWS Lambda Functions](https://docs.aws.amazon.com/lambda/latest/dg/python-programming-model.html) and [Azure Functions](https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-first-function-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying a Model as an API with Flask\n",
    "\n",
    "<a title=\"Armin Ronacher, Copyrighted free use, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Flask_logo.svg\"><img width=\"256\" alt=\"Flask logo\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Flask_logo.svg/256px-Flask_logo.svg.png\"></a>\n",
    "\n",
    "***Flask*** is a [microframework](https://flask.palletsprojects.com/en/2.0.x/foreword/#what-does-micro-mean) for web development with Python. It can be used for full-stack web applications, but it's also very popular for deploying machine learning models as APIs. In fact, the Google Cloud function Python tooling that is used in the curriculum lesson linked above uses Flask \"under the hood\"!\n",
    "\n",
    "Here is that same cloud function, rewritten as a Flask app:\n",
    "\n",
    "```python\n",
    "from flask import Flask, request\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def iris_prediction(sepal_length, sepal_width, petal_length, petal_width):\n",
    "    with open(\"model.pkl\", \"rb\") as f:\n",
    "        model = joblib.load(f)\n",
    "    X = [[sepal_length, sepal_width, petal_length, petal_width]]\n",
    "    predictions = model.predict(X)\n",
    "    prediction = int(predictions[0])\n",
    "    return {\"predicted_class\": prediction}\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def index():\n",
    "    return 'Hello, world!'\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    request_json = request.get_json()\n",
    "    result = iris_prediction(**request_json)\n",
    "    return json.dumps(result)\n",
    "```\n",
    "\n",
    "It's definitely a bit longer than the cloud function version, but it has two benefits:\n",
    "\n",
    "1. You can actually run the server on your local computer, which makes **debugging** much faster and easier.\n",
    "2. Rather than using Google Cloud functions (a paid service, although there are free credits when you sign up), you can use **Heroku** to deploy your API, which is **100% free** for up to 5 apps. (You can also deploy using an EC2 instance or other cloud container if your model is too large/slow for Heroku.)\n",
    "\n",
    "Check out these curriculum lessons for more info: [Introduction to Flask](https://github.com/learn-co-curriculum/dsc-flask-intro), [Deploying a Model with Flask](https://github.com/learn-co-curriculum/dsc-flask-deployment).\n",
    "\n",
    "Besides Heroku and AWS EC2, you can also host flask apps with [Azure App Service](https://docs.microsoft.com/en-us/learn/modules/host-a-web-app-with-azure-app-service/index), [DigitalOcean](https://www.digitalocean.com/community/tutorials/how-to-deploy-a-flask-application-on-an-ubuntu-vps), [Google Cloud App Engine](https://cloud.google.com/python/getting-started/hello-world), and [AWS Elatic Beanstalk](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying a Model as a Full-Stack Web App\n",
    "\n",
    "An API interface is very useful, but you also might want something more interactive and impressive for your data science portfolio. Developing a ***full-stack web app*** means that there is a web page interface, so once your app is deployed, someone can load it directly in the browser and generate model predictions by clicking on the page.\n",
    "\n",
    "#### Deploying a Model as a Full-Stack Web App with Flask\n",
    "\n",
    "<img src=\"images/model_view_controller.png\" width=600 alt=\"model view controller diagram\" />\n",
    "<small><i>Icons made by <a href=\"https://www.flaticon.com/authors/freepik\" target=\"blank_\">Freepik</a> from www.flaticon.com</i></small>\n",
    "\n",
    "The same microframework described for deploying a model as an API can also be used to make a full-stack web app. This requires that you write HTML and CSS (and optionally JavaScript) as well as Python in order to create the \"view\" component of the model-view-controller (MVC) framework.\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* If you already have experience with HTML and CSS, this approach can allow you to flex your creativity and make something very polished that shows off all of your skills. You can add as many pages as you want (e.g. to create a portfolio website rather than a single-page application)\n",
    "* You can generate data visualizations in Python with Matplotlib (don't need to learn any new plotting libraries)\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* If you don't have any experience with HTML and CSS, the learning curve can be steep. These languages don't produce straightforward error messages like Python does, so it can be quite difficult to know where your mistakes are\n",
    "* Unless you know JavaScript and are prepared to work with a library like [D3.js](https://d3js.org/), your visualizations aren't going to be interactive\n",
    "\n",
    "If you are interested in using this approach, check out this [template repository](https://github.com/learn-co-students/capstone-flask-app-template-082420), which includes instructions in the README for modifying the HTML and Python code so that it will work for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying a Model as a Full-Stack Web App with Dash\n",
    "\n",
    "<img src=\"images/dash_app.png\" />\n",
    "\n",
    "***Plotly Dash*** (also just referred to as Dash) is \"the most downloaded, trusted framework for building machine learning web apps in Python\" ([source](https://plotly.com/building-machine-learning-web-apps-in-python/)). It allows you to create interactive websites that make predictions from machine learning models, all without writing HTML, CSS, or JavaScript directly.\n",
    "\n",
    "Dash is built on top of Flask, so it can be deployed using Heroku as well. Check out [this link](https://calm-bastion-07515.herokuapp.com/) for an example app hosted on Heroku (might take a while to load if it hasn't been used in a while), and these curriculum lessons for more info: [Introduction to Dash](https://github.com/learn-co-curriculum/dsc-dash-intro), [Deploying a Model with Dash](https://github.com/learn-co-curriculum/dsc-dash-deployment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up: Pickling a Model for Deployment Demo\n",
    "\n",
    "This shows the basic outline for training a model, evaluating it, then using it in a \"production\" context to make predictions about new data.\n",
    "\n",
    "### Model Training and Evaluation\n",
    "\n",
    "We'll use the wine dataset from scikit-learn for a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn imports\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# get premade wine dataset from sklearn\n",
    "data = load_wine()\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build a model to predict the class of wine\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0, n_estimators=100)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# get the model accuracy\n",
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the confusion matrix\n",
    "metrics.confusion_matrix(y_test, classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling the Model\n",
    "\n",
    "The [`pickle` format](https://docs.python.org/3/library/pickle.html) is built into the Python language. It's called pickling because it is a form of preserving an object in memory for later. This is achieved by converting everything about the Python variable into bits of data that can be stored in a file.\n",
    "\n",
    "For scikit-learn models, [the `joblib` library is recommended instead](https://scikit-learn.org/stable/modules/model_persistence.html). This works similarly to `pickle` but has built-in functionality that works better with NumPy arrays.\n",
    "\n",
    "In the cell below, we take our `classifier` variable and turn it into a file on disk called `wine_classifier.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# use the built-in open() function to open a file\n",
    "output_file = open(\"wine_classifier.pkl\", \"wb\") # \"wb\" means \"write as bytes\"\n",
    "# dump the variable's contents into the file\n",
    "joblib.dump(classifier, output_file)\n",
    "# close the file, ensuring nothing stays in the buffer\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model\n",
    "\n",
    "This part would actually almost never be in the same file as the previous step. The goal is to take information that was stored in memory at one time, then save it so it can be used later. Here specifically this is useful because training a model is usually a lot slower than using the model to make a prediction, so this saves us from having to re-run that costly operation each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the built-in open() function again, this time to read\n",
    "model_file = open(\"wine_classifier.pkl\", \"rb\") # \"rb\" means \"read as bytes\"\n",
    "# load the variable's contents from the file into a variable\n",
    "loaded_model = joblib.load(model_file)\n",
    "# close the file\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Prediction with the Loaded Model\n",
    "\n",
    "In this section I'm constructing a request JSON that resembles what would come from a user who wants a predicted class of wine based on these feature values. This code would not actually exist in your deployed application, it would be created automatically by whatever protocol generated the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a fake request JSON from the user with all the headings\n",
    "\n",
    "request_json = {}\n",
    "\n",
    "expected_features = (\"Alcohol\", \"Malic acid\", \"Ash\", \"Alcalinity of ash\", \\\n",
    "        \"Magnesium\", \"Total phenols\", \"Flavanoids\", \"Nonflavanoid phenols\", \\\n",
    "        \"Proanthocyanins\", \"Color intensity\", \"Hue\", \\\n",
    "        \"OD280/OD315 of diluted wines\", \"Proline\")\n",
    "example_values = [1.282e+01, 3.370e+00, 2.300e+00, 1.950e+01, 8.800e+01, 1.480e+00, \\\n",
    "       6.600e-01, 4.000e-01, 9.700e-01, 1.026e+01, 7.200e-01, 1.750e+00, \\\n",
    "       6.850e+02]\n",
    "\n",
    "for i, feature in enumerate(expected_features):\n",
    "    request_json[feature] = example_values[i]\n",
    "request_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the section that more closely resembles what you might have in your application. I'm checking to make sure that the expected values are in the request_json, transforming them into the right format to make a prediction, then printing out that prediction. In your actual deployed code, you would most likely be **returning** the response, not printing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if request_json and all(feature in request_json for feature in expected_features):\n",
    "    \n",
    "    # unpack all of the relevant values from the request into a list\n",
    "    \n",
    "    test_value = [request_json[feature] for feature in expected_features]\n",
    "    \n",
    "    # make a prediction from the \"user input\"\n",
    "    \n",
    "    predicted_class = int(loaded_model.predict([test_value])[0])\n",
    "    \n",
    "    # construct a response\n",
    "    \n",
    "    response_json = {\"prediction\": predicted_class}\n",
    "    print(response_json)\n",
    "else:\n",
    "    print(\"something was missing from the request\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more extended explanation of pickling, check out these curriculum lessons: [Pickle](https://github.com/learn-co-curriculum/dsc-pickle) and [Pickling and Deploying Pipelines](https://github.com/learn-co-curriculum/dsc-pickling-pipelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up: AWS S3 Buckets with Boto3 Demo\n",
    "\n",
    "For the purpose of this example, the `wine_classifier.pkl` file has already been uploaded to the Flatiron School Curriculum AWS account. To make your own pickle file available from code, you would need to make an account and upload the file.\n",
    "\n",
    "### Optional Prerequisite: CLI Interface\n",
    "\n",
    "Installation instructions [here](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html), CLI docs [here](https://docs.aws.amazon.com/cli/latest/reference/s3/).  You will need to use this to upload large files (somewhere around 160 MB) but it's clunkier than integrating directly into Python and won't work with all deployment techniques.\n",
    "\n",
    "If you want to upload a document using the CLI interface, that will look something like this:\n",
    "\n",
    "```bash\n",
    "aws s3 cp s3://<your bucket name>/wine_classifier.pkl wine_classifier.pkl\n",
    "```\n",
    "\n",
    "### Using Boto3 to Retrieve a Pickled Model\n",
    "\n",
    "![AWS CLI plus Python equals Boto3](https://curriculum-content.s3.amazonaws.com/data-science/images/boto3.png)\n",
    "\n",
    "Boto 3 is a library that allows Python developers to access many different Amazon web services, not just S3. You can find the full list [here](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/index.html). But we'll focus on using S3 with boto3 because that is one of the most common use cases for data scientists.\n",
    "\n",
    "If you are accessing public resources in an S3 bucket, the interface is pretty simple. For example, here is how you would load the pickled wine classifier from an S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# instantiate a connection to the S3 resource\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "# load an object from that resource\n",
    "pkl_obj = s3.Object(bucket_name=\"curriculum-content\", key=\"data-science/wine_classifier.pkl\")\n",
    "\n",
    "# get the response (under the hood this is a similar to `requests`)\n",
    "pkl_resp = pkl_obj.get()[\"Body\"].read()\n",
    "\n",
    "# look at what's in there (first 100 characters)\n",
    "pkl_resp[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Pickled Model from Boto3\n",
    "\n",
    "As you can see from the print-out above, we have a `RandomForestClassifier`. But right now it's still encoded as a string of bytes rather than loaded into a scikit-learn model.\n",
    "\n",
    "To load it into an actual model, we'll need to use `BytesIO` (a class in the built-in [Python `io` module](https://docs.python.org/3/library/io.html)) and then we'll be able to load it with `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this so we'll ignore warnings about our scikit-learn version being different\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "# read the string of bytes into BytesIO object\n",
    "pkl_bytes = BytesIO(pkl_resp)\n",
    "\n",
    "# load the model using joblib\n",
    "boto3_loaded_model = joblib.load(pkl_bytes)\n",
    "boto3_loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display model params\n",
    "boto3_loaded_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_loaded_model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Boto3 to Retrieve a CSV\n",
    "\n",
    "The process for loading a CSV is essentially the same as loading a pickled model, except that you can pass the `BytesIO` object directly to `pandas`, rather than needing `joblib` to perform an additional loading step first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load another object (same bucket name, different key this time)\n",
    "csv_obj = s3.Object(bucket_name=\"curriculum-content\", key=\"data-science/data/wine.csv\")\n",
    "\n",
    "# get the response\n",
    "csv_resp = csv_obj.get()[\"Body\"].read()\n",
    "\n",
    "# look at what's in there (first 100 characters)\n",
    "csv_resp[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the string of bytes into BytesIO object\n",
    "csv_bytes = BytesIO(csv_resp)\n",
    "\n",
    "# read the csv file into a dataframe using pandas\n",
    "boto3_loaded_df = pd.read_csv(csv_bytes)\n",
    "boto3_loaded_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
