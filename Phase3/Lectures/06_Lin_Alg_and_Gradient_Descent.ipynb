{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math for Data Scientists: Linear Algebra and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seaborn import load_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "gems = load_dataset('diamonds')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Correctly add and multiply matrices\n",
    "- Explain and use the concept of a gradient\n",
    "- Explain the algorithm of gradient descent\n",
    "- Describe the effect of the \"learning rate\" in the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://www.analyticsvidhya.com/blog/2019/07/10-applications-linear-algebra-data-science/) is a useful resource on the applications of linear algebra to data science. We'll explore a few here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors and Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of linear algebra, a single number is a 0-dimensional entity called a **scalar**. But it is often useful to have data in the form of a 1-dimensional object called a **vector**, which can be thought of as a list of scalars. Think here of a `pandas` Series. And in addition to the values that compose the vector, we can characterize the vector as a whole as having a **magnitude** and a **direction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot([0, 3], [0, 4], 'b')\n",
    "ax.vlines(1, ymin=0, ymax=4/3)\n",
    "ax.scatter(0, 0, s=30)\n",
    "\n",
    "# arrowhead!\n",
    "ax.vlines(3, ymin=3.7, ymax=4, colors='b')\n",
    "ax.hlines(4, xmin=2.8, xmax=3, colors='b')\n",
    "ax.annotate('$\\\\theta$', xy=(1.1, 0.5))\n",
    "ax.annotate('|v| = 5', xy=(0.9, 2.3))\n",
    "ax.set_xlim(right=5)\n",
    "ax.set_ylim(top=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been working all along with arrays and data frames that have a tabular structure of rows and columns. Such a 2-dimensional structure of numerical elements is known in linear algebra as a **matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gems.head().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want 3- or even higher-dimensional objects. Think for example of a digital image where we record the red, green, and blue values *for each pixel in the 2d array*. The linear algebraic abstraction we need for such an object is called a **tensor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "tensor = np.round(np.random.rand(3, 5, 5), 1)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor[0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices can be added and multiplied, and there are other distinctive operations on matrices that are often useful.\n",
    "\n",
    "<details>\n",
    "    <summary><b>Matrix Addition</b>: Click for Illustration</summary>\n",
    "$\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} + b_{11} & a_{12} + b_{12} \\\\\n",
    "a_{21} + b_{21} & a_{22} + b_{22}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "my_matrix1 = np.random.randint(low=1, high=11, size=(2, 2))\n",
    "my_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matrix2 = np.random.randint(low=1, high=11, size=(2, 2))\n",
    "my_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matrix1 + my_matrix2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Matrix Multiplication</b>: Click for Illustration</summary>\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_{1,1} & b_{1,2} \\\\\n",
    "b_{2,1} & b_{2,2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{1,1}\\times b_{1,1} + a_{1,2}\\times b_{2,1} & a_{1,1}\\times b_{1,2} + a_{1,2}\\times b_{2,2} \\\\\n",
    "a_{2,1}\\times b_{1,1} + a_{2,2}\\times b_{2,1} & a_{2,1}\\times b_{1,2} + a_{2,2}\\times b_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_matrix1.dot(my_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Python short hand\n",
    "my_matrix1 @ my_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not the same as 'multiplication'\n",
    "my_matrix1 * my_matrix2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application: Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can express covariance and correlation matrices as linear-algebraic transformations:\n",
    "\n",
    "For a centered data matrix $M$:\n",
    "- $cov(M) = \\frac{1}{n-1}M^TM$, where $n$ is the number of observations.\n",
    "\n",
    "A centered data matrix is one whose column means are all 0.\n",
    "\n",
    "This equation makes use of the **transpose** of a matrix $M$, $M^T$, which is the matrix that results from swapping the rows and columns of $M$. You can also think of this as a *reflection* of the elements of $M$ about the main diagonal of $M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matrix1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matrix_transposed = my_matrix1.T\n",
    "my_matrix_transposed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate this equation. Suppose we have ten observations (rows) for each of three variables (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "mat_1 = np.random.rand(10, 3)\n",
    "mat_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_1_centered = mat_1 - np.mean(mat_1, axis=0)\n",
    "\n",
    "mat_1_centered.T.dot(mat_1_centered) / 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cov = np.cov(mat_1, rowvar=False)\n",
    "\n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(mat_1, rowvar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Bonus: Correlation Matrices</summary>\n",
    "    To calculate a correlation matrix, we can multiply the covariance matrix on both sides by a diagonal matrix of the reciprocals of the standard deviations of the columns. Source: https://blogs.sas.com/content/iml/2010/12/10/converting-between-correlation-and-covariance-matrices.html.\n",
    "\n",
    "<code>stds = np.sqrt(np.diag(cov))\n",
    "np.diag(stds\\*\\*-1).dot(cov).dot(np.diag(stds\\*\\*-1))\n",
    "np.corrcoef(mat_1, rowvar=False)\n",
    "</code>\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression and Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now a typical dataset and the associated multiple linear regression problem. We have many observations (rows), each of which consists of a set of values both for the predictors (columns, i.e. the independent variables) and for the target (the dependent variable).\n",
    "\n",
    "For the equation $A\\vec{x} = \\vec{c}$, we can think of the values of the independent variables (i.e. the data matrix, \"X\") as our matrix $A$ and the vector of coefficients as $\\vec{x}$ and of the values of the dependent variable (i.e. the target, \"y\") as our output vector $\\vec{c}$.\n",
    "\n",
    "The task here is, in effect, to solve for $\\vec{\\beta}$, where we have that $A\\vec{\\beta} = \\vec{c}$, except in general we'll have more rows than columns. But more rows than columns means more equations than unknowns, which means that in general **there is no solution**. This is why instead we go for an optimization--in our case, a best-fit line. So we have $A\\vec{\\beta}\\approx\\vec{c}$.\n",
    "\n",
    "Using $a$ for our independent variables and $c$ for our dependent variable, we have:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta_1\\begin{bmatrix}\n",
    "a_{1,1} \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "a_{m,1}\n",
    "\\end{bmatrix} +\n",
    "... + \\beta_n\\begin{bmatrix}\n",
    "a_{1,n} \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "a_{m,n}\n",
    "\\end{bmatrix} \\approx \\begin{bmatrix}\n",
    "c_1 \\\\\n",
    ".  \\\\\n",
    ".  \\\\\n",
    ".  \\\\\n",
    "c_m\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Algebra Solves the Best-Fit Line Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a matrix of predictors $X$ and a target column $y$, we can express $\\vec{\\beta}$, the vectorized parameters of the best-fit line, as  follows:\n",
    "\n",
    "$\\large\\vec{\\beta} = (X^TX)^{-1}X^Ty$.\n",
    "\n",
    "$(X^TX)^{-1}X^T$ is sometimes called the *pseudo-inverse* of $X$.\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "x = np.array(list(zip(np.random.normal(size=10),\n",
    "                          np.array(np.random.normal(size=10, loc=2)))))\n",
    "y = np.array(np.random.exponential(size=10))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x= sm.add_constant(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Not square, won't work\n",
    "np.linalg.solve(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.linalg.inv(x.T.dot(x)).dot(x.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LinearRegression(fit_intercept=True).fit(x, y).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LinearRegression(fit_intercept=True).fit(x, y).intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm.OLS(y, x).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is a technique from *calculus* that underlies the operation of many machine learning modeling algorithms. We'll use it here to approach the regression problem of finding the line of best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Solving the Line of Best Fit by Guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's say we have some data below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Randomly created data in x & y\n",
    "np.random.seed(27)\n",
    "\n",
    "x = np.random.rand(30, 1).reshape(30)\n",
    "y_randterm = np.random.normal(0, 3, 30)\n",
    "y = 3 + 50 * x + y_randterm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here's the data plotted out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(x, y)\n",
    "\n",
    "ax.set_title('Data Points to Model')\n",
    "ax.set_xlabel('x', fontsize=14)\n",
    "ax.set_ylabel('y', fontsize=14)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 60)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we wanted to make a best-fit line, what would you guess? Let's create a couple functions to make this easier to make a guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8,
     24
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plotting a guess of a regression line\n",
    "def regression_formula(x, a, b):\n",
    "    return a*x + b\n",
    "\n",
    "def plot_data_and_guess(slope, intercept, ax, x1=x, x2=y, **kwargs):\n",
    "    '''\n",
    "    Plot our data and regression line on the given axis.\n",
    "\n",
    "    Arguments:\n",
    "        slope : float\n",
    "            Value for the slope the regression line.\n",
    "            \n",
    "        intercept : float\n",
    "            Value for the intercept the regression line.\n",
    "        \n",
    "        ax : Axes\n",
    "            Axis to plot data and regression line\n",
    "        \n",
    "        x1 : array-like\n",
    "            Values along the x-axis\n",
    "        \n",
    "        x2 : array-like\n",
    "            Values along the y-axis\n",
    "        \n",
    "    Returns:\n",
    "        fig : Figure\n",
    "\n",
    "        ax : Axes\n",
    "    '''\n",
    "    # Plot data and regression line\n",
    "    ax.scatter(x1, x2)\n",
    "    yhat = regression_formula(x1, slope, intercept)\n",
    "    ax.plot(x1, yhat, 'r-', **kwargs)\n",
    "    \n",
    "    # Embelishments\n",
    "    ax.set_title('Data Points to Model')\n",
    "    ax.set_xlabel('x', fontsize=14)\n",
    "    ax.set_ylabel('y', fontsize=14)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 60)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So what do you think the regression parameters are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Our guess\n",
    "guess = {\n",
    "    'slope': 30,\n",
    "    'intercept': 0\n",
    "}\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_data_and_guess(**guess, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What would be your next guess be? \n",
    "\n",
    "- How can we tell when our guess is \"better\"?\n",
    "- Could we formalize this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### The Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One way we can know how well our guess or _model_ did is to compare the predicted values with the actual values. These are the _residuals_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So this would give us the error for each data point:\n",
    "\n",
    "$$ r_i = \\hat{y}_i - y_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculate_residuals(x_values, y_values, slope, intercept):\n",
    "    '''Find the residulas for each data point'''\n",
    "    yhat = intercept + slope*x_values\n",
    "    errors = y_values - yhat\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then we can go further by having just one number to represent how faithful our model was to the actual y-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This leads us to the idea of the **mean squared error** or **MSE**. This is all the residuals squared and then averaged:\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i}^{n} (\\hat{y}_i - y_i)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mse(x_values, y_values, slope, intercept):\n",
    "    \n",
    "    resid_sq = calculate_residuals(x_values, y_values, slope, intercept)**2 \n",
    "\n",
    "    return sum(resid_sq) / len(x_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use our guess from earlier\n",
    "slope = guess.get('slope')\n",
    "intercept = guess.get('intercept')\n",
    "\n",
    "mse(x, y, slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> The function we use to find how bad our model did in prediction is typically called the **loss function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What we found here is great! We can now compare different models with one another.\n",
    "\n",
    "If we made a few different guesses, we could make our predictions and then calculate from the _loss function_ how good or bad our model did! We will want to find the _smallest loss_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### The Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now our model changes based on the different model _parameters_ (the coefficients $\\beta_i$ for linear regression). \n",
    "\n",
    "If we imagine all the different ways we can adjust these parameters $\\vec{\\theta}$ and measure how well the model performs with the loss or **cost function** $J(\\vec{\\theta})$, we can plot this as a surface in this multidimensional plane. See the image below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/gradientdescent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note that the terms **loss function** and **cost function** are frequently used interchangeably. Sometimes they are the same function, but sometimes they differ by making changes in the cost to improve _training_ or _learning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try creating the cost function's curve/surface for just one parameter (slope) using our earlier data example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table = np.zeros((20, 2))\n",
    "# Find the MSE for different slope values\n",
    "for idx, val in enumerate(range(40, 60)):\n",
    "    table[idx, 0] = val\n",
    "    table[idx, 1] = mse(x, y, slope=val, intercept=0)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(table[:, 0], table[:, 1], '-')\n",
    "plt.xlabel(\"Slope Values\", fontsize=14)\n",
    "plt.ylabel(\"MSE\", fontsize=14)\n",
    "plt.title(\"MSE with changes to slope\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Based on this graph, what is the optimal slope value?\n",
    "\n",
    "How could we extend this to find the best slope _and_ intercept combination?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Better Way of Guessing: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So this probably all sounds great! We just need to find the minimum of the cost function!\n",
    "\n",
    "But there's some bad news; we don't usually know what the cost function (which can be complicated!) \"looks\" like without trying a whole lot of different parameters $\\vec{\\theta}$. We'd need an _infinite_ number of parameter combinations to know $J(\\vec{\\theta})$ completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So what can we do?\n",
    "\n",
    "Well, we can take one \"guess\" (set of  parameters) and then measure $J(\\vec{\\theta})$. Then we can adjust our guess/parameters in a \"good\" direction, \"down the hill\". This is the basic idea of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Gradient descent** is an optimization procedure that uses the _gradient_ (a generalized notion of a derivative) of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So how do we find this \"better\" guess? Well, we need to find the best direction to move \"downhill\" the fastest. We can do this with a generalization of the derivative called the **gradient**:\n",
    "\n",
    "$$\\begin{align}\\\\\n",
    "    \\large -\\nabla J &= -\\sum_i \\dfrac{\\partial J}{\\partial \\theta_i}\\hat{\\theta_i} \\\\\n",
    "            &= -\\frac{\\partial J}{\\partial \\theta_1}\\hat{\\theta_1} + \\dots +  \\frac{\\partial J}{\\partial \\theta_n}\\hat{\\theta_n}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the multivariate case, the gradient tells us how the function is changing **in each dimension**. A large value of the derivative with respect to a particular variable means that the gradient will have a large component in the corresponding direction. Therefore, **the gradient will point in the direction of steepest increase**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Gradient Descent in Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Make a guess at where the function attains its minimum value\n",
    "- Calculate the gradient/derivative at that point\n",
    "- Use that value to decide how to make your next guess!\n",
    "\n",
    "Repeat until we get the derivative as close as we like to 0.\n",
    "\n",
    "If we want to improve our guess at the minimum of our loss function, we'll move in the **opposite direction** of the gradient away from our last guess. Hence we are using the *gradient* of our loss function to *descend* to the minimum value of the relevant loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Stepping Down a Hill: Step Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So we now have the basic idea of gradient descent of \"going down a hill\" and hopefully it's obvious that the steeper the hill, the more we can adjust our parameters to get to \"bottom\" (optimal parameters) faster.\n",
    "\n",
    "But a big question is how big of a step do we take?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> The amount we adjust our parameter is determined by our **step size**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If our steps are _too big_, we risk skipping over the minimum value (optimal parameters).\n",
    "\n",
    "If our steps are _too small_, it might take us too long to reach the minimum value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![learning_rate](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here's an elegant solution: Make the size of your step **proportional to the value of the derivative at the point where you currently are in parameter space**! If we're very far from the minimum, then our values will be large, and so we therefore ca safely take a large step; if we're close to the minimum, then our values will be small, and so we should therefore take a smaller step.\n",
    "\n",
    "I said the size of the step is proportional to the value of the derivative. The constant of proportionality is often called the **\"learning rate\"**. \n",
    "\n",
    "This page helps to explain the dangers of learning rates that are too large and too small: https://www.jeremyjordan.me/nn-learning-rate/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note there are other optimizations we can do for gradient descent that rely on adjusting our cost function or how we take steps or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The general algorithm looks like this:\n",
    "\n",
    "We'll make a guess, $\\vec{s}$, at where our loss function attains a minimum. If we're not happy with how close the value of the gradient there is to 0, then we'll make a new guess, and the new guess will be constructed as follows:\n",
    "\n",
    "$\\large\\vec{s}_{new} = \\vec{s}_{old} - \\alpha\\nabla f(\\vec{s}_{old})$,\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "In the one-dimensional case, we'll have:\n",
    "\n",
    "$\\large x_{new} = x_{old} - \\alpha\\frac{df}{dx}|_{x_{old}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Gradient Descent Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's go back to our original example and implement gradient descent to find the optimal parameters (slope and intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(x, y)\n",
    "\n",
    "ax.set_title('Data Points to Model')\n",
    "ax.set_xlabel('x', fontsize=14)\n",
    "ax.set_ylabel('y', fontsize=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First we need to find the gradient for the cost function (2-dimensions: $a$ & $b$; slope & intercept):\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial b}\\frac{1}{n}\\Sigma(y_i - (b + ax_i))^2 = -\\frac{2}{n}\\Sigma (y_i-ax_i - b)$$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial a}\\frac{1}{n}\\Sigma(y_i - (b + ax_i))^2 = -\\frac{2}{n}\\Sigma x_i (y_i-ax_i - b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's formalize this into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def partial_deriv(a, b, x_i, y_i, respect_to):\n",
    "    '''\n",
    "    Get the partial derivative for cost function with respect to slope (a) \n",
    "    or intercept (b).\n",
    "    '''\n",
    "    if respect_to == 'b': # intercept\n",
    "        return (y_i - (a * x_i + b))\n",
    "    elif respect_to == 'a': # slope\n",
    "        return (x_i * (y_i - (a * x_i + b)))\n",
    "    else:\n",
    "        print('Choose either respect_to: a or b ')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe: In the code above we've left out both the factors of two and the averages!\n",
    "\n",
    "- We'll take care of the averages below, but this is easily done because **the derivative of a sum is equal to the sum of the derivatives**: $\\frac{d}{dx}[f(x) + g(x)] = \\frac{df}{dx} + \\frac{dg}{dx}$.\n",
    "\n",
    "- The factors of two won't make any difference to our goals. Very often the cost function associated with some modeling task will be something like MSE and so have a squared term, and so then when we differentiate it we'll gain a factor of two. Clearly, minimizing $f(\\beta)$ and minimizing $2f(\\beta)$ will yield the same optimal $\\beta$, and so it's often convenient to leave off the factor of two from the expression of the derivative and so minimize the **half mean squared error** function: $\\frac{1}{2}\\Sigma(y - \\hat{y})^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next let's define the step we take (amount we adjust the parameters by) using the gradient and learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def step_gradient(a, b, x, y, learning_rate):\n",
    "    db = 0\n",
    "    da = 0 \n",
    "    # For each data point, update the derivative for the slope & intercept\n",
    "    N = len(x)\n",
    "    for i in range(N):\n",
    "        \n",
    "        # Partial derivatives of loss/cost function with respect to b & a\n",
    "        # Here's where we're taking our averages. Notice that we're leaving\n",
    "        # off the factors of 2.\n",
    "        db +=  -(1/N) * partial_deriv(a, b, x[i], y[i], respect_to='b')\n",
    "        da +=  -(1/N) * partial_deriv(a, b, x[i], y[i], respect_to='a')\n",
    "        \n",
    "    # Adjust the slope & intercept by the gradient\n",
    "    new_b = b - (learning_rate * db)\n",
    "    new_a = a - (learning_rate * da)\n",
    "    \n",
    "    return (new_a, new_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try it out and keep track of our guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "guesses = []\n",
    "alpha = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Our guess\n",
    "guess = {\n",
    "    'slope': 60,\n",
    "    'intercept': 10\n",
    "}\n",
    "\n",
    "guesses.append(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_data_and_guess(**guess, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "step = step_gradient(guess['slope'], guess['intercept'], x, y, learning_rate=alpha)\n",
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mse(x, y, guess['slope'], guess['intercept'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's update our guess and try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our guess using the new step\n",
    "new_slope, new_intercept = step\n",
    "guess = {\n",
    "    'slope': new_slope,\n",
    "    'intercept': new_intercept\n",
    "}\n",
    "guesses.append(guess)\n",
    "\n",
    "# Getting adjusted parameters\n",
    "step = step_gradient(guess['slope'], guess['intercept'], x, y, learning_rate=alpha)\n",
    "display(step)\n",
    "display(mse(x, y, guess['slope'], guess['intercept']))\n",
    "\n",
    "# Plotting out our new parameters\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_data_and_guess(**guess, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's repeat this another 200 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    # Our guess using the new step\n",
    "    new_slope, new_intercept = step\n",
    "    guess = {\n",
    "        'slope': new_slope,\n",
    "        'intercept': new_intercept\n",
    "    }\n",
    "    guesses.append(guess)\n",
    "\n",
    "    # Getting adjusted parameters\n",
    "    step = step_gradient(guess['slope'], guess['intercept'], x, y, learning_rate=alpha)\n",
    "    #  Only display every 10\n",
    "    if (i % 10) == 0:\n",
    "        print(f'Step # {i}:')\n",
    "        display(step)\n",
    "        display(mse(x, y, guess['slope'], guess['intercept']))\n",
    "        print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What does our final result look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting out our new parameters\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_data_and_guess(**guesses[-1], ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's take a look at the MSE over the guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mses = [\n",
    "    mse(x, y, d['slope'], d['intercept']) for d in guesses\n",
    "]\n",
    "plt.plot(range(len(mses)), mses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This was what we had for 200 iterations. What could we do to improve or speed up this method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up: Distance Calculations\n",
    "\n",
    "There are many times when we need to measure distances. For example, many modeling algorithms rely on a notion of **similarity** between data points. But we have already seen how distance is used to construct a linear model: Choose the betas that minimize the sum of squared **distances** between true and predicted $y$-values.\n",
    "\n",
    "Consider this distance for *all* data points at once: We can think of that as a vector: $\\vec{(y_i - \\hat{y_i})^2}$.\n",
    "\n",
    "And in fact there are multiple ways to measure the magnitude of a vector. Typically, we are thinking of Euclidean spaces and so use the **L2 norm** to measure the magnitude of a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vec = np.array([3, 4, 12])\n",
    "\n",
    "np.sqrt(3**2 + 4**2 + 12**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(my_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are other norms we can use. In general, the **$n$-norm** will calculate $(x_1^n + ... + x_m^n)^{\\frac{1}{n}}$ for a vector $\\vec{x_i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(my_vec, ord=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter Minkowski Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Can take the form of many distances](https://en.wikipedia.org/wiki/Minkowski_distance)\n",
    "\n",
    "[Blog post](https://www.kdnuggets.com/2023/03/distance-metrics-euclidean-manhattan-minkowski-oh.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up: Eigenvalues, Singular Values, Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to express a matrix as a **product** of other matrices. Sometimes the gain is only in computational efficiency, but there are also certain factorizations or **decompositions** that are useful in other ways.\n",
    "\n",
    "An **eigendecomposition** reduces a matrix to a collection of vectors that capture the *linear* action of the matrix. Selecting the vectors that produce the largest such linear transformations is the idea behind **principal component analysis**, which is useful for reducing high-dimensional datasets to lower-dimensional problems.\n",
    "\n",
    "Eigendecompositions are possibly only for square matrices; a **singular value decomposition** is a more fundamental matrix factorization that can be applied to any matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do recommendation engines work?\n",
    "\n",
    "Imagine representing your interests (film genres, book subjects, music styles) as a **vector**: larger numbers represent larger preferences. Now do this for multiple people. Now we can think of comparing these vectors directly or against some target such as whether a given product/service was used/bought/watched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our data is **unlabeled** we have a problem in **unsupervised learning**. One major strategy for this type of problem is to impose a *similarity* metric on our data points. Similarity between data points is measured as some function of the **(vector) distance** between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One similarity metric for vectors is **cosine similarity**, which computes the *cosine of the angle between them*. Note that this is always well-defined for non-zero vectors since any two vectors determine a plane (in which the angle can be measured)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw already above the idea of representing a digital image as a **tensor** of values that encode facts about each pixel in the digitization.\n",
    "\n",
    "**Neural networks** are good for working with tensors of high dimension. Such objects often need to be manipulated into different shapes, and the `.reshape()` method is great for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bringing back our tensor variable from above\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.reshape(5, 15)\n",
    "\n",
    "tensor.reshape(1, 75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
