{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d96924a",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "One of the common issues when building a machine learning model for prediction is \"**overfitting**\". To reduce the risk of overfitting, data scientists use different strategies to adjust the model, so it can be better used to predict unseen data.\n",
    "\n",
    "Overfitting:\n",
    "When do we expect to see an overfitting issue from a ML model?\n",
    "\n",
    "It usually happens when the model is fitting the training data very well (low bias), but not so well for predicting the testing data (high variance). Especially when we are building the OLS linear regression model, the objective of the model is to minimize the sum of squared residual (error), therefore, the estimated parameters are by design to yield the lowest bias with the given dataset.\n",
    "\n",
    "Now, the question is if the OLS model is having such low bias fitting the training dataset, does it have a good predictive power for unseen data? In many cases, it is not necessary to be true.  So, how can we use the bias and variance trade-off to create a better predictive model based on the OLS linear regression foundation?\n",
    "\n",
    "The answer is \"Regularization\"!!\n",
    "\n",
    "1. Lasso Regression (L1)  \n",
    "2. Ridge Regression (L2)  \n",
    "3. ElasticNet (L1 + L2)  \n",
    "\n",
    "The idea behind \"**Regularization**\" is to add a little more **bias** to our model, so it could have lower **variance** to predict the unseen data.\n",
    "\n",
    "Note: The regularization is not designed for interpretation, but to improve predictive power of the model, so the estimated coefficients should not be interpreted directly nor perform statistical inference (hypothesis test).\n",
    "\n",
    "### Lasso Regression\n",
    "\n",
    "The general concept for Lasso Regression is to add a penalty to a well-fitted OLS model (smaller $\\beta$s), so the variance of the model would decrease for better prediction of the unseen data. Think about the loss function between the OLS and Lasso regression models,\n",
    "\n",
    "OLS:\n",
    "\n",
    "$$SSE = \\sum{(y_i - \\beta_0 - \\beta_1 x_i)^2}$$\n",
    "\n",
    "Losso:\n",
    "\n",
    "$$SSE = \\sum{(y_i - \\beta_0 - \\beta_1 x_i)^2} + \\lambda |\\beta_1|$$\n",
    "\n",
    "where $\\lambda$ represents the weight of the penalty to the estimated coefficient in the OLS model. The absolute value of $\\beta_1$ is the term we are trying to adjust in the model.  Since the estimated slope coefficient $\\beta_1$ could be positive or negative, so we use absolute value to ensure that it will punish the loss function by adding more error to the model.\n",
    "\n",
    "The ultimate goal is to minimize the loss function by adjusting the slope coefficient, so the model will have higher level of variance. However, this adjustment will make the model with higher bias as a trade-off.\n",
    "\n",
    "\n",
    "#### Validating the Model:\n",
    "\n",
    "For Lasso regression, we use CV error to validate the value of $\\lambda$, so we can use the correct penalty on the estimated parameter for the best balance of bias and variance that yields the lowest error.\n",
    "\n",
    "Python Code:\n",
    "\n",
    "Note: Because \"*lambda*\" is a special keyword in Python, so the parameter name in Lasso, Ridge, and ElasticNet regression function is replaced by \"*alpha*\".\n",
    "\n",
    "``` Python\n",
    "# Import dependencies for lasso regression\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Create the lasso regression object\n",
    "lasso = Lasso(alpha=100, random_state=123456)\n",
    "\n",
    "# Fit the data to the model object\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Predict with the training data\n",
    "y_hat = lasso.predict(X)\n",
    "\n",
    "# Calculate the goodness of fit of the model, R^2\n",
    "lasso.score(X, y)\n",
    "\n",
    "# Calculate the mean squared error of the model\n",
    "mean_squared_error(y_hat, y)\n",
    "\n",
    "# Calculate the cross-validation R^2 and mse\n",
    "cv = cross_validation(X=X, y=y, estimator=lasso, cv=8,\n",
    "                      scoring=('r2', 'neg_mean_squared_error'),\n",
    "                      return_train_score=True)\n",
    "\n",
    "# Check the CV train R^2\n",
    "cv['train_r2']\n",
    "\n",
    "# Check the CV test R^2\n",
    "cv['test_r2']\n",
    "\n",
    "# Create the regularized regression with cross-validation\n",
    "losso_cv = LossoCV(cv=8)\n",
    "\n",
    "# Fit the training the data with the model\n",
    "losso_cv.fit(X, y)\n",
    "\n",
    "# Calculate the goodness of fit of the model, R^2\n",
    "losso_cv.score(X, y)\n",
    "\n",
    "# Calculate the mean squared error of the model\n",
    "mean_squared_error(y, losso_cv.predict(X))\n",
    "```\n",
    "\n",
    "#### When to use Lasso Regression?\n",
    "\n",
    "By design of the loss function, the estimated slope parameters could be driven to 0 for minimum error condition, which mean the parameters could be eliminated from the model. Therefore, if you know that you modeling with some useless predictors or features from your dataset in the model, you should consider using Lasso Reqression as a strategy for feature selection to simplify your model specification.\n",
    "\n",
    "Try to build the Lasso Regression model with following dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b625f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T14:52:38.250827Z",
     "start_time": "2021-09-01T14:52:36.970706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41573157",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T14:56:55.571097Z",
     "start_time": "2021-09-01T14:56:53.873461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TotalValue</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>NumFloors</th>\n",
       "      <th>UnitsTotal</th>\n",
       "      <th>LotFront</th>\n",
       "      <th>LotDepth</th>\n",
       "      <th>BldgFront</th>\n",
       "      <th>BldgDepth</th>\n",
       "      <th>BuiltFAR</th>\n",
       "      <th>ResidFAR</th>\n",
       "      <th>CommFAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>327600.0</td>\n",
       "      <td>769</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3</td>\n",
       "      <td>19.00</td>\n",
       "      <td>53.92</td>\n",
       "      <td>19.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>5.34</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>943650.0</td>\n",
       "      <td>1512</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>36.17</td>\n",
       "      <td>46.67</td>\n",
       "      <td>36.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4.94</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>897300.0</td>\n",
       "      <td>2180</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>34.92</td>\n",
       "      <td>69.75</td>\n",
       "      <td>34.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2.81</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>914400.0</td>\n",
       "      <td>2275</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>42.17</td>\n",
       "      <td>55.25</td>\n",
       "      <td>41.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.57</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>927900.0</td>\n",
       "      <td>1885</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2</td>\n",
       "      <td>29.00</td>\n",
       "      <td>66.92</td>\n",
       "      <td>29.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>4.90</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TotalValue  LotArea  NumFloors  UnitsTotal  LotFront  LotDepth  BldgFront  \\\n",
       "0    327600.0      769        4.5           3     19.00     53.92       19.0   \n",
       "1    943650.0     1512        5.0           7     36.17     46.67       36.0   \n",
       "2    897300.0     2180        3.0           3     34.92     69.75       34.0   \n",
       "3    914400.0     2275        4.0           3     42.17     55.25       41.0   \n",
       "4    927900.0     1885        5.5           2     29.00     66.92       29.0   \n",
       "\n",
       "   BldgDepth  BuiltFAR  ResidFAR  CommFAR  \n",
       "0       54.0      5.34      10.0     15.0  \n",
       "1       44.0      4.94      10.0     15.0  \n",
       "2       69.0      2.81      10.0     15.0  \n",
       "3       63.0      3.57      10.0     15.0  \n",
       "4       66.0      4.90      10.0     15.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data from the web into pd dataframe\n",
    "path = 'https://jaredlander.com/data/manhattan_Train.csv'\n",
    "manhattan = pd.read_csv(path)\n",
    "manhattan = manhattan[['TotalValue', 'LotArea', 'NumFloors', 'UnitsTotal',\n",
    "                       'LotFront', 'LotDepth', 'BldgFront', 'BldgDepth',\n",
    "                       'BuiltFAR', 'ResidFAR', \"CommFAR\"]]\n",
    "manhattan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec46b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to code the Lasso Regression here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35146796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4636f477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f08ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2de2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39349cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3331b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "997cd34c",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "Ridge Regression has the same concept as to the Lasso Regression. The idea is to adjust the slope coefficient to add more bias to the model with the return of lower variance for predicting unseen data. The only different is, instead of using the absolute value of the slope coefficient, we squared the slope coefficient in the loss function.\n",
    "\n",
    "OLS:\n",
    "\n",
    "$$SSE = \\sum{(y_i - \\beta_0 - \\beta_1 x_i)^2}$$\n",
    "\n",
    "Losso:\n",
    "\n",
    "$$SSE = \\sum{(y_i - \\beta_0 - \\beta_1 x_i)^2} + \\lambda |\\beta_1|$$\n",
    "\n",
    "Ridge:\n",
    "\n",
    "$$SSE = \\sum{(y_i - \\beta_0 - \\beta_1 x_i)^2} + \\lambda \\beta_1^2$$\n",
    "\n",
    "where $\\lambda$ again represents the penalty weight on the estimated slope coefficient, $\\beta_1$.\n",
    "\n",
    "In most cases, Ridge Regression is preferred over Lasso Regression. Therefore, if you wonder which one to use for building a predictive model, it could be a safe option to start with Ridge Regression.\n",
    "\n",
    "#### Validating the Model:\n",
    "\n",
    "For Ridge regression, we use CV error to validate the value of $\\lambda$, so we can use the correct penalty on the estimated parameter for the best balance of bias and variance that yields the lowest error.\n",
    "\n",
    "Python Code:\n",
    "\n",
    "``` Python\n",
    "# Import dependencies for lasso regression\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Create the lasso regression object\n",
    "ridge = Ridge(alpha=100, random_state=123456)\n",
    "\n",
    "# Fit the data to the model object\n",
    "ridge.fit(X, y)\n",
    "\n",
    "# Predict with the training data\n",
    "y_hat = ridge.predict(X)\n",
    "\n",
    "# Calculate the goodness of fit of the model, R^2\n",
    "ridge.score(X, y)\n",
    "\n",
    "# Calculate the mean squared error of the model\n",
    "mean_squared_error(y_hat, y)\n",
    "\n",
    "# Calculate the cross-validation R^2 and mse\n",
    "cv = cross_validation(X=X, y=y, estimator=ridge, cv=8,\n",
    "                      scoring=('r2', 'neg_mean_squared_error'),\n",
    "                      return_train_score=True)\n",
    "\n",
    "# Check the CV train R^2\n",
    "cv['train_r2']\n",
    "\n",
    "# Check the CV test R^2\n",
    "cv['test_r2']\n",
    "\n",
    "# Create the regularized regression with cross-validation\n",
    "ridge_cv = RidgeCV(cv=8)\n",
    "\n",
    "# Fit the training the data with the model\n",
    "ridge_cv.fit(X, y)\n",
    "\n",
    "# Calculate the goodness of fit of the model, R^2\n",
    "ridge_cv.score(X, y)\n",
    "\n",
    "# Calculate the mean squared error of the model\n",
    "mean_squared_error(y, ridge_cv.predict(X))\n",
    "```\n",
    "\n",
    "#### When to use Ridge Regression?\n",
    "\n",
    "By design of the loss function, the estimated parameters can only be driven to very close to 0 (cannot be exactly 0). Therefore, it is the best situation to use Ridge Regression, when you know most of the predictors or features you are using in the model are useful to predict the target value, so you only reducing the weight of the slope parameters, instead of eliminating them from your model.\n",
    "\n",
    "Try the code for building the Ridge Regression with the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d878269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to build the Ridge Regression here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6eca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae805c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7528dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1402e01e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "493b2034",
   "metadata": {},
   "source": [
    "### ElasticNet\n",
    "\n",
    "Finally, ElasticNet is a combination of both the Lasso and Ridge Regression models. The concept of ElasticNet is to minimize a loss function, which is the sum of the errors from both the Lasso and Ridge regression. A specific feature in ElsaticNet is to adjust the weight parameter $\\rho$, which represents the weight of Lasso Regression the loss function. \n",
    "\n",
    "Weight Parameter: $\\rho$\n",
    "\n",
    "$\\rho = 0.1$ means that the Lasso Regression will have weight of 10% in the loss function and 90% weight is from the Ridge Regression.\n",
    "\n",
    "$\\rho = 0.5$ means that the Lasso and Ridge Regressions both have 50% weight in the loss function.\n",
    "\n",
    "ElasticNet:\n",
    "\n",
    "$$SSE = \\rho \\sum{(y_i - \\beta_0 - \\beta_1 x_i)^2} + \\lambda |\\beta_1| + (1 - \\rho) \\sum{(y_i - \\beta_0 - \\beta_1 x_i)^2} + \\lambda \\beta_1^2$$\n",
    "\n",
    "\n",
    "Python Code:\n",
    "    \n",
    "``` Python\n",
    "# Import dependencies for ElasticNet regression\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Create the ElasticNet object, with p = 0.5\n",
    "en = ElasticNet(alpha=100, 11_ratio=0.5, random_state=123456)\n",
    "\n",
    "# Fit the data to the model object\n",
    "en.fit(X, y)\n",
    "\n",
    "# Predict with the training data\n",
    "y_hat = en.predict(X)\n",
    "\n",
    "# Calculate the goodness of fit of the model, R^2\n",
    "en.score(X, y)\n",
    "\n",
    "# Calculate the mean squared error of the model\n",
    "mean_squared_error(y_hat, y)\n",
    "\n",
    "# Calculate the cross-validation R^2 and mse\n",
    "cv = cross_validation(X=X, y=y, estimator=en, cv=8,\n",
    "                      scoring=('r2', 'neg_mean_squared_error'),\n",
    "                      return_train_score=True)\n",
    "\n",
    "# Check the CV train R^2\n",
    "cv['train_r2']\n",
    "\n",
    "# Check the CV test R^2\n",
    "cv['test_r2']\n",
    "\n",
    "# Create the regularized regression with cross-validation\n",
    "en_cv = ElasticNetCV(cv=8)\n",
    "\n",
    "# Fit the training the data with the model\n",
    "en_cv.fit(X, y)\n",
    "\n",
    "# Calculate the goodness of fit of the model, R^2\n",
    "en_cv.score(X, y)\n",
    "\n",
    "# Calculate the mean squared error of the model\n",
    "mean_squared_error(y, en_cv.predict(X))\n",
    "```\n",
    "\n",
    "#### When to use ElasticNet?\n",
    "\n",
    "You can use ElasticNet in any situation, but setting $\\rho$ is a challenge for beginners and often we need to check with CV error to confirm the best $\\rho$ to use for a particular problem.\n",
    "\n",
    "Let's try to build the ElasticNet model and compares to the previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac3255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to build the ElasticNet Regression here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1cf30a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e4b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1fe049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
