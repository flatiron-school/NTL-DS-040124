{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple and Multiple Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AKA - Welcome to statistical modeling! Could also say - welcome to **Supervised Machine Learning**.\n",
    "\n",
    "What do I mean by 'Supervised' ?\n",
    "\n",
    "![Types of machine learning, broken down](images/machinelearning_supervisedunsupervised.png)\n",
    " \n",
    "[Image Source](https://fr.mathworks.com/help/stats/machine-learning-in-matlab.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's Goals:\n",
    "\n",
    "- Recognize the importance of model validation\n",
    "- Implement a train-test split\n",
    "- Evaluate a simple linear regression model\n",
    "- Add additional variables to create a multiple linear regression model\n",
    "- Scale features appropriately, based on training data, for a multiple linear regression model\n",
    "\n",
    "Bonus, if we have time: other useful metrics for regression models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First: Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Data visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Modeling!\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit data from https://www.kaggle.com/avikpaul4u/credit-card-balance\n",
    "\n",
    "Target: `Balance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "df = pd.read_csv('data/Credit.csv', \n",
    "                 usecols=['Income', 'Limit', 'Rating', 'Cards', 'Age', 'Balance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation - AKA How to Build Generalizable Models\n",
    "\n",
    "![validation gif from giphy](https://media.giphy.com/media/242wLqQerWkxd6GgHB/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bias-Variance Trade Off\n",
    "\n",
    "<img alt=\"original image from https://rmartinshort.jimdofree.com/2019/02/17/overfitting-bias-variance-and-leaning-curves/\" src=\"images/underfit-goodfit-overfit.png\" width=750, height=350>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Minimize Bias and Variance\n",
    "\n",
    "### Combat Underfitting (Bias)\n",
    "\n",
    "**Bias**: Error introduced by approximating a real-life problem (which may be extremely complicated) by a much simpler model (because the model is too simple to capture the underlying pattern)\n",
    "\n",
    "**The Solution:** evaluate the performance of our models, using a scoring metric, which will help us catch if a model is underfit - if it's performing quite poorly, it probably isn't capturing the relationship in our data! \n",
    "\n",
    "### Combat Overfitting (Variance)\n",
    "\n",
    "**Variance**: Amount by which our model would change if we estimated it using a different training dataset (because the model is over-learning from the training data)\n",
    "\n",
    "**The Solution:** don't train your model on ALL of your data, but keep some of it in reserve to test on, in order to simulate how it will work on new/incoming data.\n",
    "\n",
    "\n",
    "<img alt=\"original image from https://www.dataquest.io/wp-content/uploads/kaggle_train_test_split.svg plus some added commentary\" src=\"images/traintestsplit_80-20.png\" width=650, height=150>  \n",
    "\n",
    "How does this fight against overfitting? By witholding data from the training process, we are testing whether the model actually _generalizes_ well. If it does poorly on the test set, it's a good sign that our model learned too much noise from the train set and is overfit! \n",
    "\n",
    "![arrested development gif, found by Andy](https://heavy.com/wp-content/uploads/2013/05/tumblr_mjm9fqhrle1rvnnvyo6_250.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice:\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the train_test_split function from sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to define our X and y\n",
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split here!\n",
    "# Set test_size = .33\n",
    "# Set random_state = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did that do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train + X_test) == len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOU SHOULD ALWAYS START WITH A TRAIN TEST SPLIT.**\n",
    "\n",
    "**FOR THE PROJECT, YOU WILL BE _REQUIRED_ TO WORK WITH A TRAIN TEST SPLIT**\n",
    "\n",
    "Note - for the checkpoints and code challenge, follow the instructions given - they might not require a train/test split as they attempt to keep thing simple.\n",
    "\n",
    "BUT we're going to use it in this session! Let's see what this looks like in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For statsmodels, we'll create a train_df and test_df\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "test_df = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the training data to make any modeling decisions!\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "Let's start off with one variable - which should we choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to evaluate our options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our choice:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your formula\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and fit your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check your results!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate!\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now what?\n",
    "\n",
    "We have a trained model... what can we do with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our predictions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Just looking at two variables... we can visualize this!\n",
    "\n",
    "# Plot our points as a scatterplot\n",
    "\n",
    "# Plot the line of best fit!\n",
    "\n",
    "# plt.ylabel('')\n",
    "# plt.xlabel('')\n",
    "# plt.title('')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to our actual train values...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can score our models without relying on the statsmodels output!\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use sklearn to score our model, too:\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Score our training data - the same score as from our summary!\n",
    "# This function requires two inputs: y_true and y_pred:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can now predict for our test set!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Score our testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One last thing - can visualize both train and test set!\n",
    "\n",
    "# Plot our training data\n",
    "\n",
    "# Plot our testing data\n",
    "\n",
    "\n",
    "# Plot the line of best fit\n",
    "\n",
    "# Plotting for the test data just to show it's the same!\n",
    "\n",
    "# plt.ylabel('')\n",
    "# plt.xlabel('')\n",
    "# plt.title('')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Same as simple linear regression, but with more inputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our formula\n",
    "# First, define a list of X columns...\n",
    "X_cols = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... Then use that list to make our formula, using join\n",
    "formula = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up and fit your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check your results!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation time!\n",
    "\n",
    "How'd we do? What looks different from the simple linear regression output? What in the world can we do with those coefficients?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization, AKA Feature Scaling and Centering\n",
    "\n",
    "Scaling data is the process of **increasing or decreasing the magnitude according to a fixed ratio.** You change the size but not the shape of the data. Often, this involves dividing features by their standard deviation.\n",
    "\n",
    "Centering also does not change the shape of the data, but instead simply **removes the mean value  of each feature** so that each is centered around zero instead of their original mean.\n",
    "\n",
    "The idea is that you can standardize data so that a model can interpret each individual feature more consistently.\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler\n",
    "\n",
    "We've seen one of these options before - when else did we remove the mean and then divide by the standard deviation?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating our scaler\n",
    "stdscaler = None\n",
    "\n",
    "# Creating scaled versions of one column\n",
    "age_scaled = stdscaler.fit_transform(train_df[['Age']]) # pass Age col as a df\n",
    "# Why fit_transform? We'll discuss in a second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize them!\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(10,4))\n",
    "# Original data\n",
    "ax1.hist(train_df['Age'], bins=20)\n",
    "ax1.set_title(\"Original Age Data\")\n",
    "# Scaled data\n",
    "ax2.hist(age_scaled, bins=20)\n",
    "ax2.set_title(\"Standard Scaled Age Data\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Changed?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need to use feature scaling?\n",
    "\n",
    "- In order to compare the magnitude of coefficients thus increasing the interpretability of coefficients\n",
    "- Handling disparities in units\n",
    "- Some models use euclidean distance in their computations\n",
    "- Some models require features to be on equivalent scales\n",
    "- In the machine learning space, it helps improve the performance of the model and reducing the values/models from varying widely\n",
    "- Some algorithms are sensitive to the scale of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit_transform` ?\n",
    "\n",
    "Note above we used `fit_transform` - why is that? **Consistency!**\n",
    "\n",
    "`fit` allows the scaler to learn the patterns _only from the training data_! \n",
    "\n",
    "Why does that matter?\n",
    "\n",
    "- mean and standard deviation are affected by the data in the rows - so, if we fit to the whole dataset instead of just the training data, the scaler would learn patterns influenced by the test data!\n",
    "\n",
    "`transform` allows the scaler to apply the pattern it learns - can do so on both the train and test sets, so long as they're equivalent (aka have the same columns and were prepared the same way!)\n",
    "\n",
    "**ALWAYS FIT ON TRAINING DATA, THEN TRANSFORM THE TRAIN AND TEST SETS!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Try It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new scaler\n",
    "\n",
    "# Learn the pattern from the training data\n",
    "\n",
    "#Apply the pattern to the training and testing data\n",
    "X_train_scaled = None\n",
    "X_test_scaled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is this object?\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's that look like as a dataframe?\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled,\n",
    "                              columns=X_train.columns,\n",
    "                              index=X_train.index)\n",
    "\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make train_df_scaled, which includes the target variable, for statsmodels\n",
    "train_df_scaled = pd.concat([X_train_scaled, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our formula stays the same\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and fit your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your results!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate:\n",
    "\n",
    "What changed?\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "### Also - how do we interpret these coefficients, or those p-values in the summary?\n",
    "\n",
    "Discuss:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step?\n",
    "\n",
    "What could we do next to improve this model?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out a potential issue in our model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important note to keep in mind from now on:\n",
    "\n",
    "![\"all models are wrong but some are useful\" quote picture](images/allmodelsarewrong.jpg)\n",
    "\n",
    "[Image Source](https://twitter.com/cwodtke/status/1244433603666178049)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources:\n",
    "\n",
    "- [Excellent statistical writeup about how to interpret Linear Regression coefficients, and their p-values](https://statisticsbyjim.com/regression/interpret-coefficients-p-values-regression/)\n",
    "- [Great bias/variance infographic](https://elitedatascience.com/bias-variance-tradeoff) from Elite Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit: Beyond the $R^2$ Score\n",
    "\n",
    "There are other metrics! \n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "$$\\text{MAE}(y, y_\\text{pred}) = \\frac{1}{n} \\sum_{i=0}^{n} \\left| y_i - y_\\text{pred}i \\right|$$\n",
    "\n",
    "- Measures the average magnitude of errors regardless of direction, by calculating the total absolute value of errors and dividing by the number of samples (number of predictions made)\n",
    "- **This error term is in the same units as the target!**\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "$$\\text{MSE}(y, y_\\text{pred}) = \\frac{1}{n} \\sum_{i=0}^{n} (y_i - y_\\text{pred}i)^2$$\n",
    "\n",
    "- Measures the average squared error, by calculating the sum of squared errors for all predictions then dividing by the number of samples (number of predictions)\n",
    "- In other words - this is the Residual Sum of Squares (RSS) divided by the number of predictions!\n",
    "- This error term is **NOT** in the same units as the target!\n",
    "\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "$$\\text{RMSE}(y, y_\\text{pred}) = \\sqrt{\\frac{1}{n} \\sum_{i=0}^{n} (y_i - y_\\text{pred}i)^2}$$\n",
    "\n",
    "- Measures the square root of the average squared error, by calculating the sum of squared errors for all predictions then dividing by the number of samples (number of predictions), then taking the square root of all that\n",
    "- **This error term is in the same units as the target!**\n",
    "\n",
    "Note - before, we were _maximizing_ R2 (best fit = largest R2 score). But we'd want to minimize these other error metrics.\n",
    "\n",
    "Documentation: \n",
    "- [Regression Metrics in sklearn](https://scikit-learn.org/stable/modules/classes.html#regression-metrics)\n",
    "- [User Guide for Regression Metrics in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab training set predictions\n",
    "train_preds = model_scaled.predict(X_train_scaled)\n",
    "train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metrics:\")\n",
    "# R2\n",
    "print(f\"R2: {r2_score(y_train, train_preds):.3f}\")\n",
    "# MAE\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_train, train_preds):.3f}\")\n",
    "# MSE\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_train, train_preds):.3f}\")\n",
    "# RMSE - just MSE but set squared=False\n",
    "print(f\"Root Mean Squared Error: {mean_squared_error(y_train, train_preds, squared=False):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I said that MAE and RMSE are both in the same units as our target, but you'll see that they are different here. What's the difference?\n",
    "\n",
    "> \"Taking the square root of the average squared errors has some interesting implications for RMSE. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable.\"\n",
    "\n",
    "-- Source: [\"MAE and RMSE — Which Metric is Better?\"](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we interpret these?\n",
    "\n",
    "- R2: \"Our model accounts for 61.2% of the variance in our target\"\n",
    "- MAE/RMSE: \"Our model's predictions are, on average, about __ off from our actual target values\" (here, balance is likely in dollars - so $___ off)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
